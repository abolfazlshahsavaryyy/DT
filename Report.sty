I explored the Kaggle website to find a proper dataset. After searching, I found one titled "Flight Passenger Satisfaction."
I created a GitHub repository for this project and added the relevant files.

Using Colab, I read the dataset as a DataFrame (df) and cleaned it. After cleaning, I had a df with no non-integer values and classified the continuous columns.

After cleaning df, I split the data into train, test, and valid sets and then separated the train, test, and valid sets into X and y for each one.
I decided to cancel this project from here because I didn't know that my dataset was unused.

I explored Kaggle again and found another dataset for predicting diabetes based on age, BMI, etc.
First, I cleaned it. It had already been mapped to numerical classes, so it didn't need further mapping.
I classified the continuous columns.
I used the BMI classification chart to get a more correct prediction.

After cleaning the data, I wrote the model class to create the DecisionTree model.
After checking the output of the model (predictions), I found that my data was too simple, and I couldn't get good accuracy.
(The highest accuracy was only 74% on the validation set and 76% on the training set.)

I decided to change my dataset again, and after changing the dataset, cleaning it, and creating the model, I found that the accuracy was too high.
So, I explored the Kaggle website and found that it was normal. At that time, I was actually underfitting with 98% accuracy.

After performing grid search on the model, I found the best model with an accuracy of 99.60%.

The reason the accuracy is so high is that my dataset is used for detecting the type of attack or normal request on a website.
We have a lot of parameters, and these parameters divide normal requests from attack requests.
Because of this, attack requests always have some identifying signs that make them easy to detect.

but I head to another problem witch entropy what to lower that gini the reson was that this model has some feature
and those feature are near 90% same value and it devide the very well this has two reson to increase the IG
first of all the lower value seperate the tree and the other part miss extra part so the IG was to higjt 

the best way to solve this problem was in my oppinian drop dose values because the nois was to hight in this 
features after deleteing this bad feature that not split very well the model with entropy increate the accuracy 

