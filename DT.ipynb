{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7pWem6s-aSb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQzfSIz--aSd"
      },
      "outputs": [],
      "source": [
        "def Classify_continuous(df:pd.DataFrame,col_name ):\n",
        "\n",
        "    col=df[col_name]\n",
        "    min=np.min(col)\n",
        "    max=np.max(col)\n",
        "    mean=np.mean([max,min])\n",
        "    mean_75=np.mean([max,mean])\n",
        "    mean_25=np.mean([min,mean])\n",
        "    df.loc[df[col_name]<=min,col_name]=0\n",
        "    df.loc[(df[col_name]>min )& (df[col_name]<=mean_25),col_name]=1\n",
        "    df.loc[(df[col_name]>mean_25) & (df[col_name]<=mean),col_name]=2\n",
        "    df.loc[(df[col_name]>mean) & (df[col_name]<=mean_75),col_name]=3\n",
        "    df.loc[(df[col_name]>mean_75) & (df[col_name]<=max),col_name]=4\n",
        "    df.loc[df[col_name]>max ,col_name]=5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMZP0ZHc-aSe"
      },
      "outputs": [],
      "source": [
        "def clean_data(df:pd.DataFrame):\n",
        "\n",
        "\n",
        "    # drop the non coreleation columns\n",
        "    df=df.drop(columns=[\"hot\",\"is_guest_login\",\"logged_in\",\"is_host_login\",\"num_outbound_cmds\",\"target\",\"num_failed_logins\",\"land\",\"num_root\",\"wrong_fragment\",\"su_attempted\",\"num_file_creations\",\"is_guest_login\",\"urgent\",\"is_host_login\",\"num_outbound_cmds\",\"num_access_files\",\"num_shells\"])\n",
        "\n",
        "    df.loc[(df[\"duration\"]<=0),\"duration\"]=0\n",
        "    df.loc[(df[\"duration\"]>0),\"duration\"]=1\n",
        "    map_protocol_type={\n",
        "        \"icmp\":0,\n",
        "        \"tcp\":1,\n",
        "        \"udp\":2\n",
        "    }\n",
        "    df[\"protocol_type\"]=df[\"protocol_type\"].map(map_protocol_type)\n",
        "\n",
        "    map_service = {\"ecr_i\": 0, \"smtp\": 1, \"http\": 2, \"private\": 3,\"domain_u\":4}\n",
        "    df[\"service\"] = df[\"service\"].map(map_service).fillna(5).astype(int)\n",
        "\n",
        "    map_flag={\"SF\":0,\"S0\":1,\"REJ\":2,\"RSTO\":4,\"RSTR\":3}\n",
        "    df[\"flag\"]=df[\"flag\"].map(map_flag).fillna(5).astype(int)\n",
        "    col_continuos=[\"src_bytes\",\"diff_srv_rate\",\"dst_host_same_src_port_rate\"]\n",
        "    for s in col_continuos:\n",
        "        Classify_continuous(df,col_name=s)\n",
        "\n",
        "    df.loc[(df[\"dst_bytes\"]==0),\"dst_bytes\"]=0\n",
        "    df.loc[(df[\"dst_bytes\"]!=0),\"dst_bytes\"]=1\n",
        "\n",
        "    df.loc[(df[\"num_compromised\"]==0),\"num_compromised\"]=0\n",
        "    df.loc[(df[\"num_compromised\"]!=0),\"num_compromised\"]=1\n",
        "\n",
        "    df.loc[(df[\"count\"]>20),\"count\"]=1\n",
        "    df.loc[(df[\"count\"]<=20),\"count\"]=0\n",
        "\n",
        "    df.loc[(df[\"srv_count\"]>20),\"srv_count\"]=1\n",
        "    df.loc[(df[\"srv_count\"]<=20),\"srv_count\"]=0\n",
        "\n",
        "\n",
        "    df.loc[(df[\"serror_rate\"]==0),\"serror_rate\"]=0\n",
        "    df.loc[(df[\"serror_rate\"]!=0),\"serror_rate\"]=1\n",
        "\n",
        "    df.loc[(df[\"srv_serror_rate\"]==0),\"srv_serror_rate\"]=0\n",
        "    df.loc[(df[\"srv_serror_rate\"]!=0),\"srv_serror_rate\"]=1\n",
        "\n",
        "    df.loc[(df[\"rerror_rate\"]==0),\"rerror_rate\"]=0\n",
        "    df.loc[(df[\"rerror_rate\"]!=0),\"rerror_rate\"]=1\n",
        "\n",
        "    df.loc[(df[\"srv_rerror_rate\"]==0),\"srv_rerror_rate\"]=0\n",
        "    df.loc[(df[\"srv_rerror_rate\"]!=0),\"srv_rerror_rate\"]=1\n",
        "\n",
        "    df.loc[(df[\"same_srv_rate\"]==1),\"same_srv_rate\"]=0\n",
        "    df.loc[(df[\"same_srv_rate\"]!=1),\"same_srv_rate\"]=1\n",
        "\n",
        "    df.loc[(df[\"srv_diff_host_rate\"]==0),\"srv_diff_host_rate\"]=0\n",
        "    df.loc[(df[\"srv_diff_host_rate\"]!=0),\"srv_diff_host_rate\"]=1\n",
        "\n",
        "    df.loc[(df[\"dst_host_count\"]==255),\"dst_host_count\"]=0\n",
        "    df.loc[(df[\"dst_host_count\"]!=255),\"dst_host_count\"]=1\n",
        "\n",
        "    df.loc[(df[\"dst_host_srv_count\"]==255),\"dst_host_srv_count\"]=0\n",
        "    df.loc[(df[\"dst_host_srv_count\"]!=255),\"dst_host_srv_count\"]=1\n",
        "\n",
        "\n",
        "    df.loc[(df[\"dst_host_same_srv_rate\"]==1),\"dst_host_same_srv_rate\"]=0\n",
        "    df.loc[(df[\"dst_host_same_srv_rate\"]!=1),\"dst_host_same_srv_rate\"]=1\n",
        "\n",
        "    df.loc[(df[\"dst_host_diff_srv_rate\"]==0),\"dst_host_diff_srv_rate\"]=0\n",
        "    df.loc[(df[\"dst_host_diff_srv_rate\"]!=0),\"dst_host_diff_srv_rate\"]=1\n",
        "\n",
        "    df.loc[(df[\"dst_host_srv_diff_host_rate\"]==0),\"dst_host_srv_diff_host_rate\"]=0\n",
        "    df.loc[(df[\"dst_host_srv_diff_host_rate\"]!=0),\"dst_host_srv_diff_host_rate\"]=1\n",
        "\n",
        "    df.loc[(df[\"dst_host_srv_rerror_rate\"]==0),\"dst_host_srv_rerror_rate\"]=0\n",
        "    df.loc[(df[\"dst_host_srv_rerror_rate\"]!=0),\"dst_host_srv_rerror_rate\"]=1\n",
        "\n",
        "    df.loc[(df[\"dst_host_rerror_rate\"]==0),\"dst_host_rerror_rate\"]=0\n",
        "    df.loc[(df[\"dst_host_rerror_rate\"]!=0),\"dst_host_rerror_rate\"]=1\n",
        "\n",
        "    df.loc[(df[\"dst_host_srv_serror_rate\"]==0),\"dst_host_srv_serror_rate\"]=0\n",
        "    df.loc[(df[\"dst_host_srv_serror_rate\"]!=0),\"dst_host_srv_serror_rate\"]=1\n",
        "\n",
        "    df.loc[(df[\"dst_host_serror_rate\"]==0),\"dst_host_serror_rate\"]=0\n",
        "    df.loc[(df[\"dst_host_serror_rate\"]!=0),\"dst_host_serror_rate\"]=1\n",
        "\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zjt7syzy-aSe"
      },
      "outputs": [],
      "source": [
        "def count_values_columns(df:pd.DataFrame,cols:list):\n",
        "    for s in cols:\n",
        "        print(df[s].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2ItsyGd-aSf"
      },
      "outputs": [],
      "source": [
        "def split_X_y(df,y_label=\"class\"):\n",
        "    y=df[y_label]\n",
        "    X=df.drop(columns=[y_label])\n",
        "    return X,y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPPU2BlF-aSf"
      },
      "outputs": [],
      "source": [
        "def split_train_test_valid(df, random_state=42, test_size=0.2, valid_size=0.2):\n",
        "    n = len(df)\n",
        "    np.random.seed(random_state)\n",
        "    indexes = np.random.permutation(n)\n",
        "\n",
        "\n",
        "    test_size_df = int(n * test_size)\n",
        "    valid_size_df = int((n - test_size_df) * valid_size)\n",
        "    train_size_df = n - (test_size_df + valid_size_df)\n",
        "\n",
        "\n",
        "    df_train = df.iloc[indexes[:train_size_df]].reset_index(drop=True)\n",
        "    df_test = df.iloc[indexes[train_size_df:train_size_df + test_size_df]].reset_index(drop=True)\n",
        "    df_valid = df.iloc[indexes[train_size_df + test_size_df:]].reset_index(drop=True)\n",
        "\n",
        "    return df_train, df_test, df_valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOYBac5q-aSf"
      },
      "outputs": [],
      "source": [
        "def gini_impurity(y):\n",
        "    _, counts = np.unique(y, return_counts=True)\n",
        "    probabilities = counts / counts.sum()\n",
        "    return 1 - np.sum(probabilities ** 2)\n",
        "\n",
        "def weighted_gini(X_feature, y):\n",
        "\n",
        "    unique_values = np.unique(X_feature)\n",
        "    gini_values = []\n",
        "\n",
        "    for value in unique_values:\n",
        "        left_split = y[X_feature == value]\n",
        "        right_split = y[X_feature != value]\n",
        "\n",
        "        gini_left = gini_impurity(left_split)\n",
        "        gini_right = gini_impurity(right_split)\n",
        "\n",
        "\n",
        "        weight_left = len(left_split) / len(y)\n",
        "        weight_right = len(right_split) / len(y)\n",
        "\n",
        "        weighted_gini = (weight_left * gini_left) + (weight_right * gini_right)\n",
        "        gini_values.append((value, weighted_gini))\n",
        "\n",
        "    return min(gini_values, key=lambda x: x[1])\n",
        "\n",
        "def best_feature_to_split(X, y):\n",
        "    best_feature = None\n",
        "    best_gini = float(\"inf\")\n",
        "    best_threshold = None\n",
        "\n",
        "    for feature in X.columns:\n",
        "        threshold, gini = weighted_gini(X[feature], y)\n",
        "\n",
        "        if gini < best_gini:\n",
        "            best_gini = gini\n",
        "            best_feature = feature\n",
        "            best_threshold = threshold\n",
        "\n",
        "    return best_feature, best_threshold, best_gini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zph9Bss-aSf"
      },
      "outputs": [],
      "source": [
        "class Node:\n",
        "\n",
        "    def __init__(self, feature=None, children=None, gini=None, entropy=None, value=None):\n",
        "        self.feature = feature\n",
        "        self.children = children or {}\n",
        "        self.gini = gini\n",
        "        self.entropy = entropy\n",
        "        self.value = value\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, criterion=\"gini\", max_depth=None, min_samples_split=2, min_samples_leaf=1):\n",
        "        self.criterion = criterion\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.root = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        full_data = pd.DataFrame(X).copy()\n",
        "        full_data[\"label\"] = y\n",
        "        self.root = self._build_tree(full_data, depth=0)\n",
        "\n",
        "    def _build_tree(self, data, depth):\n",
        "\n",
        "        y = data[\"label\"]\n",
        "        root_gini = self._gini(y)\n",
        "        root_entropy = self._entropy(y)\n",
        "        root_label=self._compute_label(y)\n",
        "        if (self.max_depth is not None and depth >= self.max_depth) or \\\n",
        "        (len(data) < self.min_samples_split) or (len(y) == 0) or \\\n",
        "        (len(y.unique()) == 1):\n",
        "            return Node(value=root_label, gini=root_gini, entropy=root_entropy)\n",
        "\n",
        "\n",
        "        best_feature = self._best_split_feature(data)\n",
        "\n",
        "        if best_feature is None:\n",
        "            return Node(value=root_label, gini=root_gini, entropy=root_entropy)\n",
        "\n",
        "        node = Node(feature=best_feature, gini=root_gini, entropy=root_entropy, children={})\n",
        "\n",
        "        for category in data[best_feature].unique():\n",
        "            subset = data[data[best_feature] == category].drop(columns=[best_feature])\n",
        "\n",
        "            if len(subset) < self.min_samples_leaf:\n",
        "                node.children[category] = Node(value=root_label, gini=root_gini, entropy=root_entropy)\n",
        "            else:\n",
        "                node.children[category] = self._build_tree(subset, depth + 1)\n",
        "\n",
        "        return node\n",
        "\n",
        "    def _best_split_feature(self, data):\n",
        "\n",
        "        best_feature = None\n",
        "        best_score = 1 if self.criterion == \"gini\" else -1\n",
        "\n",
        "        for feature in data.columns[:-1]:\n",
        "            score = self._split_score(data, feature)\n",
        "\n",
        "            if (self.criterion == \"gini\" and score < best_score) or \\\n",
        "               (self.criterion == \"entropy\" and score > best_score):\n",
        "                best_feature, best_score = feature, score\n",
        "\n",
        "        return best_feature\n",
        "    def _compute_label(self, y):\n",
        "\n",
        "        y_unique, count = np.unique(y, return_counts=True)\n",
        "        most_common_value = y_unique[np.argmax(count)]\n",
        "        return most_common_value\n",
        "        pass\n",
        "\n",
        "\n",
        "    def _split_score(self, data, feature):\n",
        "        total = len(data)\n",
        "        target = data[\"label\"]\n",
        "        weighted_impurity = 0\n",
        "\n",
        "        for category in data[feature].unique():\n",
        "            subset = data[data[feature] == category][\"label\"]\n",
        "            weight = len(subset) / total\n",
        "            impurity = self._gini(subset) if self.criterion == \"gini\" else self._entropy(subset)\n",
        "            weighted_impurity += weight * impurity\n",
        "\n",
        "        return weighted_impurity\n",
        "\n",
        "    def _gini(self, y):\n",
        "        proportions = y.value_counts(normalize=True)\n",
        "        return 1 - np.sum(proportions ** 2)\n",
        "\n",
        "    def _entropy(self, y):\n",
        "        proportions = y.value_counts(normalize=True)\n",
        "        return -np.sum(proportions * np.log2(proportions + 1e-9))\n",
        "\n",
        "    def predict(self, X):\n",
        "        return [self._predict_one(row, self.root) for _, row in pd.DataFrame(X).iterrows()]\n",
        "\n",
        "    def _predict_one(self, row, node):\n",
        "\n",
        "        if node.value is not None:\n",
        "            return node.value\n",
        "\n",
        "        feature_value = row.get(node.feature)\n",
        "\n",
        "        if feature_value in node.children:\n",
        "            return self._predict_one(row, node.children[feature_value])\n",
        "        else:\n",
        "            return self._compute_label_from_node(node)\n",
        "    def _compute_label_from_node(self, node):\n",
        "\n",
        "        values = [child.value for child in node.children.values() if child.value is not None]\n",
        "        return max(set(values), key=values.count) if values else None\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ew6-ia9x-aSg"
      },
      "outputs": [],
      "source": [
        "def accuracy(y_true, y_pred):\n",
        "    return (y_true == y_pred).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "or7_DMmP-aSg"
      },
      "outputs": [],
      "source": [
        "def change_indexes(i, j, k, t, max_i, max_j, max_k, max_t):\n",
        "    if t + 1 < max_t:\n",
        "        return i, j, k, t + 1\n",
        "\n",
        "    t = 0\n",
        "    if k + 1 < max_k:\n",
        "        return i, j, k + 1, t\n",
        "\n",
        "    k = 0\n",
        "    if j + 1 < max_j:\n",
        "        return i, j + 1, k, t\n",
        "\n",
        "    j = 0\n",
        "    if i + 1 < max_i:\n",
        "        return i + 1, j, k, t\n",
        "\n",
        "    return i, j, k, t\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWt0AD4c-aSg"
      },
      "outputs": [],
      "source": [
        "def hyperparameter_tuning(dict_values, X_train, y_train, X_valid, y_valid):\n",
        "    result = {}\n",
        "    max_valid = 0\n",
        "    best_model = None\n",
        "    i, j, k, t = 0, 0, 0, 0\n",
        "\n",
        "    max_i = len(dict_values[\"max_depth\"])\n",
        "    max_j = len(dict_values[\"min_samples_leaf\"])\n",
        "    max_k = len(dict_values[\"min_samples_split\"])\n",
        "    max_t = len(dict_values[\"criterion\"])\n",
        "\n",
        "    for _ in range(max_i * max_j * max_k * max_t):\n",
        "        max_depth = dict_values[\"max_depth\"][i]\n",
        "        min_samples_leaf = dict_values[\"min_samples_leaf\"][j]\n",
        "        min_samples_split = dict_values[\"min_samples_split\"][k]\n",
        "        criterion = dict_values[\"criterion\"][t]\n",
        "\n",
        "        model = DecisionTree(max_depth=max_depth, min_samples_leaf=min_samples_leaf,\n",
        "                             min_samples_split=min_samples_split, criterion=criterion)\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred_valid = model.predict(X_valid)\n",
        "        y_pred_train = model.predict(X_train)\n",
        "\n",
        "        train_acc = accuracy(y_train, y_pred_train)\n",
        "        valid_acc = accuracy(y_valid, y_pred_valid)\n",
        "\n",
        "        result[f\"max_depth:{max_depth}, min_samples_leaf:{min_samples_leaf}, \"\n",
        "               f\"min_samples_split:{min_samples_split}, criterion:{criterion}\"] = f\"train:{train_acc}, valid:{valid_acc}\"\n",
        "\n",
        "        i, j, k, t = change_indexes(i, j, k, t, max_i, max_j, max_k, max_t)\n",
        "\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBwSuB1Q-aSg"
      },
      "source": [
        "## start of the project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "co_6iLPI-aSh"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv(\"Dataset.csv\")\n",
        "df=clean_data(df)\n",
        "\n",
        "count_values_columns(df,df.columns.to_list())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dxtt6QsF-aSh",
        "outputId": "6ffd25e2-f6a8-4154-eb85-f512894986b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('dst_bytes', np.int64(0), np.float64(0.08431970393670667))\n"
          ]
        }
      ],
      "source": [
        "train_df,test_df,valid_df=split_train_test_valid(df,random_state=42,test_size=0.2,valid_size=0.2)\n",
        "x_train,y_train=split_X_y(train_df,y_label=\"Attack Type\")\n",
        "x_test,y_test=split_X_y(test_df,y_label=\"Attack Type\")\n",
        "x_valid,y_valid=split_X_y(valid_df,y_label=\"Attack Type\")\n",
        "print(best_feature_to_split(x_train,y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlIVJJsJ-aSi"
      },
      "outputs": [],
      "source": [
        "hyperparameter={\n",
        "    \"max_depth\":[9,10],\n",
        "    \"min_samples_leaf\":[20,25],\n",
        "    \"min_samples_split\":[20,30,35],\n",
        "    \"criterion\":[\"gini\",\"entropy\"]\n",
        "}\n",
        "res=hyperparameter_tuning(hyperparameter,x_train,y_train,x_valid,y_valid)\n",
        "res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RRguRd1-aSi",
        "outputId": "3fb8ac16-8d6d-470e-8406-7101e5db2c4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9968827172988948\n",
            "0.9969636972047038\n"
          ]
        }
      ],
      "source": [
        "#baced on the the res I choose the max_depth=9 and min_samples_leaf=20 and min_sample_split=30 and gini as the criterion\n",
        "best_model=DecisionTree(max_depth=9,min_samples_leaf=20,min_samples_split=30,criterion=\"gini\")\n",
        "best_model.fit(x_train,y_train)\n",
        "y_pred=best_model.predict(x_test)\n",
        "y_pred_train=best_model.predict(x_train)\n",
        "print(f\"test: {accuracy(y_pred,y_test)}\")\n",
        "print(f\"train: {accuracy(y_train,y_pred_train)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lvct_FN4-aSi"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}